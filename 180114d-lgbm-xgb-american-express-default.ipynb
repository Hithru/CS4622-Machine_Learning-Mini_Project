{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-13T11:02:41.955249Z","iopub.status.busy":"2023-01-13T11:02:41.954064Z","iopub.status.idle":"2023-01-13T11:02:43.309718Z","shell.execute_reply":"2023-01-13T11:02:43.306349Z","shell.execute_reply.started":"2023-01-13T11:02:41.955099Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import gc\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset_ = pd.read_feather('../input/amexfeather/train_data.ftr')\n","# Keep the latest statement features for each customer\n","train_dataset = train_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del train_dataset_\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset.info(max_cols=191,show_counts=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset.describe()"]},{"cell_type":"markdown","metadata":{},"source":["# Explore a Pattern"]},{"cell_type":"markdown","metadata":{},"source":["The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n","\n","* D_* = Delinquency variables (bad or criminal behaviour, especially among young people)\n","* S_* = Spend variables\n","* P_* = Payment variables\n","* B_* = Balance variables\n","* R_* = Risk variables\n","\n","with the following features being categorical: ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["categorical_cols = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","\n","num_cols = [col for col in train_dataset.columns if col not in categorical_cols + [\"target\"]]\n","\n","print(f'Total number of features: {1}')\n","print(f'Total number of categorical features: {len(categorical_cols)}')\n","print(f'Total number of continuos features: {len(num_cols)}')"]},{"cell_type":"markdown","metadata":{},"source":["# Visualizing Target"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sns.countplot(x = 'target', data = train_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["# Visualizing categorial features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 30))\n","for i, k in enumerate(categorical_cols):\n","    plt.subplot(6, 2, i+1)\n","    temp_val = pd.DataFrame(train_dataset[k].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n","    temp_val.index.name = 'value'\n","    temp_val.reset_index(inplace=True)\n","    plt.bar(temp_val.index, temp_val['count'], alpha=0.5)\n","    plt.xlabel(k)\n","    plt.ylabel('frequency')\n","    plt.xticks(temp_val.index, temp_val.value)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Visualizing categorial features based on the target"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20, 30))\n","for i, f in enumerate(categorical_cols):\n","    plt.subplot(6, 2, i+1)\n","    temp = pd.DataFrame(train_dataset[f][train_dataset.target == 0].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n","    temp.index.name = 'value'\n","    temp.reset_index(inplace=True)\n","    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=0')\n","    temp = pd.DataFrame(train_dataset[f][train_dataset.target == 1].value_counts(dropna=False, normalize=True).sort_index().rename('count'))\n","    temp.index.name = 'value'\n","    temp.reset_index(inplace=True)\n","    plt.bar(temp.index, temp['count'], alpha=0.5, label='target=1')\n","    plt.xlabel(f)\n","    plt.ylabel('frequency')\n","    plt.legend()\n","    plt.xticks(temp.index, temp.value)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Visualizing continuous features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for i, l in enumerate(num_cols):\n","    if i % 4 == 0: \n","        if i > 0: plt.show()\n","        plt.figure(figsize=(20, 3))\n","    plt.subplot(1, 4, i % 4 + 1)\n","    plt.hist(train_dataset[l], bins=200, color='#C69C73')\n","    plt.xlabel(l)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Aggregated profile features"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["Delinquency = [d for d in train_dataset.columns if d.startswith('D_')]\n","Spend = [s for s in train_dataset.columns if s.startswith('S_')]\n","Payment = [p for p in train_dataset.columns if p.startswith('P_')]\n","Balance = [b for b in train_dataset.columns if b.startswith('B_')]\n","Risk = [r for r in train_dataset.columns if r.startswith('R_')]\n","Dict = {'Delinquency': len(Delinquency), 'Spend': len(Spend), 'Payment': len(Payment), 'Balance': len(Balance), 'Risk': len(Risk),}\n","\n","plt.figure(figsize=(10,5))\n","sns.barplot(x=list(Dict.keys()), y=list(Dict.values()));"]},{"cell_type":"markdown","metadata":{},"source":["# Check null values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NaN_Val = np.array(train_dataset.isnull().sum())\n","NaN_prec = np.array((train_dataset.isnull().sum() * 100 / len(train_dataset)).round(2))\n","NaN_Col = pd.DataFrame([np.array(list(train_dataset.columns)).T,NaN_Val.T,NaN_prec.T,np.array(list(train_dataset.dtypes)).T], index=['Features','Num of Missing values','Percentage','DataType']\n",").transpose()\n","pd.set_option('display.max_rows', None)\n","NaN_Col"]},{"cell_type":"markdown","metadata":{},"source":["There are many missing values in the dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Drop unuseful columns"]},{"cell_type":"markdown","metadata":{},"source":["Remove columns if there are > 80% of missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = train_dataset.drop(['S_2','D_66','D_42','D_49','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["# Fill null values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selected_col = np.array(['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145'])\n","\n","for col in selected_col:\n","    train_dataset[col] = train_dataset[col].fillna(train_dataset[col].median())"]},{"cell_type":"markdown","metadata":{},"source":["In describe session you saw, lot of cloumns means are NaN. So, that's why i have used median to fill NaN values. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selcted_col2 = np.array(['D_68','B_30','B_38','D_64','D_114','D_116','D_117','D_120','D_126'])\n","\n","for col2 in selcted_col2:\n","    train_dataset[col2] =  train_dataset[col2].fillna(train_dataset[col2].mode()[0])"]},{"cell_type":"markdown","metadata":{},"source":["# Check again null values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(train_dataset.isnull().sum().to_string())"]},{"cell_type":"markdown","metadata":{},"source":["There are no more missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Load Testing DataSet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset_ = pd.read_feather('../input/amexfeather/test_data.ftr')\n","# Keep the latest statement features for each customer\n","test_dataset = test_dataset_.groupby('customer_ID').tail(1).set_index('customer_ID', drop=True).sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del test_dataset_\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset.shape"]},{"cell_type":"markdown","metadata":{},"source":["# Check null values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NaN_Val2 = np.array(test_dataset.isnull().sum())\n","NaN_prec2 = np.array((test_dataset.isnull().sum() * 100 / len(test_dataset)).round(2))\n","NaN_Col2 = pd.DataFrame([np.array(list(test_dataset.columns)).T,NaN_Val2.T,NaN_prec2.T,np.array(list(test_dataset.dtypes)).T], index=['Features','Num of Missing values','Percentage','DataType']\n",").transpose()\n","pd.set_option('display.max_rows', None)\n","\n","NaN_Col2"]},{"cell_type":"markdown","metadata":{},"source":["# Drop unuseful columns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset = test_dataset.drop(['S_2','D_42','D_49','D_66','D_73','D_76','R_9','B_29','D_87','D_88','D_106','R_26','D_108','D_110','D_111','B_39','B_42','D_132','D_134','D_135','D_136','D_137','D_138','D_142'], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["# Fill null values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selected_column = np.array(['P_2','S_3','B_2','D_41','D_43','B_3','D_44','D_45','D_46','D_48','D_50','D_53','S_7','D_56','S_9','S_12','S_17','B_6','B_8','D_52','P_3','D_54','D_55','B_13','D_59','D_61','B_15','D_62','B_16','B_17','D_77','B_19','B_20','D_69','B_22','D_70','D_72','D_74','R_7','B_25','B_26','D_78','D_79','D_80','B_27','D_81','R_12','D_82','D_105','S_27','D_83','R_14','D_84','D_86','R_20','B_33','D_89','D_91','S_22','S_23','S_24','S_25','S_26','D_102','D_103','D_104','D_107','B_37','R_27','D_109','D_112','B_40','D_113','D_115','D_118','D_119','D_121','D_122','D_123','D_124','D_125','D_128','D_129','B_41','D_130','D_131','D_133','D_139','D_140','D_141','D_143','D_144','D_145'])\n","\n","for column in selected_column:\n","    test_dataset[column] = test_dataset[column].fillna(test_dataset[column].median())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["selected_column2 = np.array(['D_68','B_30','B_38','D_114','D_116','D_117','D_120','D_126'])\n","\n","for column2 in selected_column2:\n","    test_dataset[column2] =  test_dataset[column2].fillna(test_dataset[column2].mode()[0])"]},{"cell_type":"markdown","metadata":{},"source":["# Check again null values"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(test_dataset.isnull().sum().to_string())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_dataset.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Convert categorical variable to numericals"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import OrdinalEncoder\n","\n","enc = OrdinalEncoder()\n","categorical_cols.remove('D_66')\n","\n","train_dataset[categorical_cols] = enc.fit_transform(train_dataset[categorical_cols])\n","test_dataset[categorical_cols] = enc.transform(test_dataset[categorical_cols])"]},{"cell_type":"markdown","metadata":{},"source":["# Remove highly correlated features"]},{"cell_type":"markdown","metadata":{},"source":["Remove columns if there are > 90% of correlations"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset_without_target = train_dataset.drop([\"target\"],axis=1)\n","\n","cor_matrix = train_dataset_without_target.corr()\n","col_core = set()\n","\n","for i in range(len(cor_matrix.columns)):\n","    for j in range(i):\n","        if(cor_matrix.iloc[i, j] > 0.9):\n","            col_name = cor_matrix.columns[i]\n","            col_core.add(col_name)\n","col_core"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = train_dataset.drop(col_core, axis=1)\n","test_dataset = test_dataset.drop(col_core, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset.shape"]},{"cell_type":"markdown","metadata":{},"source":["# Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_columns = [col for col in train_dataset.columns if col not in [\"target\"]]\n","\n","X = train_dataset[num_columns]\n","y = train_dataset['target']\n","\n","print(f\"X shape is = {X.shape}\" )\n","print(f\"Y shape is = {y.shape}\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","x_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","print(f\"X_train shape is = {x_train.shape}\" )\n","print(f\"Y_train shape is = {y_train.shape}\" )\n","print(f\"X_test shape is = {x_test.shape}\" )\n","print(f\"Y_test shape is = {y_test.shape}\" )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lgb_params={\n","    'learning_rate': 0.1,\n","    'objective':'regression',\n","    'importance_type':'split',\n","    'metric':'rmse',\n","    'num_leaves': 31,\n","    \"random_state\":42,\n","    'max_depth': 8,\n","    \"bagging_seed\" : 42,\n","    \"bagging_frequency\" : 5,\n","    \"reg_alpha\":0.0,\n","    \"reg_lambda\":0.0,\n","    'min_child_samples': 20,\n","    'min_child_weight':1\n","}\n","\n","xgb_params = {\n","    'objective': 'reg:squarederror',\n","    'booster': 'gbtree',\n","    'learning_rate': 0.1,\n","    'max_depth': 8,\n","    'min_child_weight': 1\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from lightgbm import LGBMRegressor\n","from xgboost import XGBRegressor\n","from sklearn.model_selection import KFold, train_test_split\n","lgbm = LGBMRegressor(**lgb_params, n_estimators=900)\n","\n","xgb = XGBRegressor(**xgb_params, n_estimators=700)\n","\n","K = 5\n","kfolds = KFold(n_splits=K, shuffle=True, random_state=42)\n","\n","for i, (train_idx, test_idx) in enumerate(kfolds.split(X)):\n","    \n","    y_train, y_valid = y.iloc[train_idx], y.iloc[test_idx]\n","    X_train, X_valid = X.iloc[train_idx, :], X.iloc[test_idx, :]\n","    \n","    print( \"\\nFold \", i)\n","    print(\"-\"* 20 + \"LGBM Regression\" + \"-\"* 20)\n","    lgbm.fit(X_train, y_train, \n","             eval_metric='rmse',\n","             eval_set=[(X_valid,  y_valid)],\n","            early_stopping_rounds=20,\n","            verbose=-1)\n","    \n","    print(\"-\"* 20 + \"XGB Regression\" + \"-\"* 20)\n","    xgb.fit(X_train, y_train, \n","             eval_metric='rmse',\n","             eval_set=[(X_valid, y_valid)],\n","            early_stopping_rounds=20,\n","            verbose=100)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Make Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lgb_preds = lgbm.predict(test_dataset[num_columns])\n","xgb_preds = xgb.predict(test_dataset[num_columns])\n","\n","boost_1, boost_2 = 0.26, 0.74\n","\n","preds_1 = boost_1 * lgb_preds\n","preds_2 = boost_2 * xgb_preds\n","\n","predictions = preds_1 + preds_2 "]},{"cell_type":"markdown","metadata":{},"source":["# Output"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_dataset = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n","output = pd.DataFrame({'customer_ID': sample_dataset.customer_ID, 'prediction': predictions})\n","output.to_csv('submission.csv', index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":4}
